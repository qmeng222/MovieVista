{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a26be7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import dotenv_values\n",
    "config = dotenv_values(\".env\")\n",
    "openai.api_key = config[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "212aeeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "import pickle\n",
    "import tiktoken\n",
    "from nomic import atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "699bb3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./movie_plots.csv\"\n",
    "dataframe = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36401906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Narrow our data set to 5000 recent American movies (to save money):\n",
    "movies = dataframe[dataframe[\"Origin/Ethnicity\"] == \"American\"].sort_values(\"Release Year\", ascending=False).head(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2c20b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the movie plots into a list:\n",
    "movie_plots = movies[\"Plot\"].values  # [\"plot1\", \"plot2\", ...] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ca63491",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "\n",
    "    # replace newlines, which can negatively affect performance:\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    return openai.Embedding.create(input=text, model=model)[\"data\"][0][\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7338d753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the tiktoken library to encode a text using the \"text-embedding-ada-002\" model\n",
    "enc = tiktoken.encoding_for_model(\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4328459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total tokens to estimate cost\n",
    "total_tokens = sum([len(enc.encode(plot)) for plot in movie_plots]) # sum([697, 757, 361, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a71ca9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated cost is $1.45\n"
     ]
    }
   ],
   "source": [
    "total_tokens\n",
    "cost = (.0004 / 1000) * total_tokens\n",
    "print(f\"Estimated cost is ${cost:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec7d8e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish a cache of embeddings to avoid recomputing\n",
    "# cache is a dict of tuples (text, model) -> embedding, saved as a pickle file\n",
    "\n",
    "# set path to embedding cache\n",
    "embedding_cache_path = \"movie_embeddings.pkl\"\n",
    "\n",
    "# load the cache if it exists, and save a copy to disk\n",
    "try:\n",
    "    embedding_cache = pd.read_pickle(embedding_cache_path)\n",
    "except FileNotFoundError:\n",
    "    embedding_cache = {}\n",
    "with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n",
    "    pickle.dump(embedding_cache, embedding_cache_file)\n",
    "\n",
    "# define a function to retrieve embeddings from the cache if present, or otherwise request via the API\n",
    "def embedding_from_string(\n",
    "    string,\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    embedding_cache=embedding_cache\n",
    "):\n",
    "    \"\"\"Return embedding of given string, using a cache to avoid recomputing.\"\"\"\n",
    "    if (string, model) not in embedding_cache.keys():\n",
    "        embedding_cache[(string, model)] = get_embedding(string, model)\n",
    "        print(f\"GOT EMBEDDING FROM OPENAI FOR '{string[:20]}'\")\n",
    "        with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n",
    "            pickle.dump(embedding_cache, embedding_cache_file)\n",
    "    return embedding_cache[(string, model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7c010c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings = [embedding_from_string(plot, model=\"text-embedding-ada-002\") for plot in movie_plots]\n",
    "# len(plot_embeddings) # 5000 \n",
    "# len(plot_embeddings[0]) # 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88592d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = movies[[\"Title\", \"Genre\"]].to_dict(\"records\")  # list of dicts: [{'Title': 'Phantom Thread', 'Genre': 'drama'}, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45e22886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-06-01 20:44:36.414\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.project\u001b[0m:\u001b[36m_create_project\u001b[0m:\u001b[36m965\u001b[0m - \u001b[1mCreating project `threatening-lashes` in organization `qmeng222`\u001b[0m\n",
      "\u001b[32m2023-06-01 20:44:38.787\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.atlas\u001b[0m:\u001b[36mmap_embeddings\u001b[0m:\u001b[36m100\u001b[0m - \u001b[1mUploading embeddings to Atlas.\u001b[0m\n",
      "4it [00:12,  3.21s/it]                             \n",
      "\u001b[32m2023-06-01 20:44:51.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.project\u001b[0m:\u001b[36m_add_data\u001b[0m:\u001b[36m1577\u001b[0m - \u001b[1mUpload succeeded.\u001b[0m\n",
      "\u001b[32m2023-06-01 20:44:51.736\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.atlas\u001b[0m:\u001b[36mmap_embeddings\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1mEmbedding upload succeeded.\u001b[0m\n",
      "\u001b[32m2023-06-01 20:44:53.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.project\u001b[0m:\u001b[36mcreate_index\u001b[0m:\u001b[36m1282\u001b[0m - \u001b[1mCreated map `threatening-lashes` in project `threatening-lashes`: https://atlas.nomic.ai/map/a7eb77dd-03de-43a2-b6ac-c7bfe6e93e6c/228d53e7-4614-4796-802a-e74278ad7ae2\u001b[0m\n",
      "\u001b[32m2023-06-01 20:44:53.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.atlas\u001b[0m:\u001b[36mmap_embeddings\u001b[0m:\u001b[36m132\u001b[0m - \u001b[1mthreatening-lashes: https://atlas.nomic.ai/map/a7eb77dd-03de-43a2-b6ac-c7bfe6e93e6c/228d53e7-4614-4796-802a-e74278ad7ae2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "project = atlas.map_embeddings(\n",
    "    embeddings=np.array(plot_embeddings), \n",
    "    data = data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d8f70a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
