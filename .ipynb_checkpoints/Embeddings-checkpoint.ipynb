{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a26be7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import dotenv_values\n",
    "config = dotenv_values(\".env\")\n",
    "openai.api_key = config[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "212aeeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "import pickle\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "699bb3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./movie_plots.csv\"\n",
    "dataframe = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36401906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Narrow our data set to 5000 recent American movies (to save money):\n",
    "movies = dataframe[dataframe[\"Origin/Ethnicity\"] == \"American\"].sort_values(\"Release Year\", ascending=False).head(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2c20b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the movie plots into a list:\n",
    "movie_plots = movies[\"Plot\"].values  # [\"plot1\", \"plot2\", ...] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ca63491",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "\n",
    "    # replace newlines, which can negatively affect performance:\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    return openai.Embedding.create(input=text, model=model)[\"data\"][0][\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05b5b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the tiktoken library to encode a text using the \"text-embedding-ada-002\" model\n",
    "enc = tiktoken.encoding_for_model(\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a43da8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total tokens to estimate cost\n",
    "total_tokens = sum([len(enc.encode(plot)) for plot in movie_plots]) # sum([697, 757, 361, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "820e6427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated cost is $1.45\n"
     ]
    }
   ],
   "source": [
    "total_tokens\n",
    "cost = (.0004 / 1000) * total_tokens\n",
    "print(f\"Estimated cost is ${cost:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f7e2b093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish a cache of embeddings to avoid recomputing\n",
    "# cache is a dict of tuples (text, model) -> embedding, saved as a pickle file\n",
    "\n",
    "# set path to embedding cache\n",
    "embedding_cache_path = \"movie_embeddings.pkl\"\n",
    "\n",
    "# load the cache if it exists, and save a copy to disk\n",
    "try:\n",
    "    embedding_cache = pd.read_pickle(embedding_cache_path)\n",
    "except FileNotFoundError:\n",
    "    embedding_cache = {}\n",
    "with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n",
    "    pickle.dump(embedding_cache, embedding_cache_file)\n",
    "\n",
    "# define a function to retrieve embeddings from the cache if present, or otherwise request via the API\n",
    "def embedding_from_string(\n",
    "    string,\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    embedding_cache=embedding_cache\n",
    "):\n",
    "    \"\"\"Return embedding of given string, using a cache to avoid recomputing.\"\"\"\n",
    "    if (string, model) not in embedding_cache.keys():\n",
    "        embedding_cache[(string, model)] = get_embedding(string, model)\n",
    "        print(f\"GOT EMBEDDING FROM OPENAI FOR '{string[:20]}'\")\n",
    "        with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n",
    "            pickle.dump(embedding_cache, embedding_cache_file)\n",
    "    return embedding_cache[(string, model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e93147f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings = [embedding_from_string(plot, model=\"text-embedding-ada-002\") for plot in movie_plots]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a1a09b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(plot_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fe489d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
